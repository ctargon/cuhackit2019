{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import random, sys, os, argparse, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neuron\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neuron_model.jpeg\" alt=\"neuron\" width=400/>\n",
    "\n",
    "The figure above shows the most fundamental unit of a neural network, the __neuron__. This unit is essentially what gives neural networks the ability to learn complex datasets. The input to the unit is a vector $\\vec{x} = \\{x_1, x_2, ..., x_n\\}$. These values are like the features (dimensions) of a dataset, which we have been using since the first notebook. The unit itself consists of a vector $\\vec{w} = \\{w_1, w_2, ..., w_3\\}$ whose values are the __weights__, with an additional value $b$ which is the __bias__. Finally, the output of the unit is the weighted sum of the input (including the bias), followed by an __activation function__. Thus, the mathematical definition of the neuron is as follows:\n",
    "\n",
    "$$a = \\sigma(\\vec{w} \\cdot \\vec{x} + b)$$\n",
    "\n",
    "__Question__: This model is actually identical to logistic regression; can you identify what model you get if you remove the activation function? _Hint: it's a model that we studied in the \"Supervised Learning\" notebook._\n",
    "\n",
    "### Activation Functions: The Secret Sauce of Success\n",
    "\n",
    "The key ingredient of the perceptron which makes it so useful is the activation function. This function is typically a nonlinear function whose output range is something small such as $[0, 1]$. One of the earliest examples of an activation function is the __sigmoid__ function:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Here's an interactive plot to give you some intuition for how the sigmoid behaves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function has two purposes in a neuron:\n",
    "\n",
    "1. Apply a nonlinear transformation to the weighted sum.\n",
    "2. \"Squash\" the weighted sum, which may be large, into a small range of possible values.\n",
    "\n",
    "The __nonlinearity__ property is especially crucial because __it allows the neuron to model nonlinear relationships__, and, as we will see later, it allows us to scale our neural network to be as large as we need it to be for a given task. Interestingly enough, the neuron model can work with all sorts of nonlinear functions, although some functions work better than others.\n",
    "\n",
    "The __squashing__ (or __saturation__) property actually comes from the neuron's original conception as a model of a biological neuron. The idea is that a biological neuron essentially behaves like a switch with a smooth transition in the middle, so even if the weighted sum of the inputs is extremely high or extremely low, the final output of the neuron will still be a simple \"on\" or \"off\". That being said, not all activation functions adhere to this squashing property as strictly as sigmoid, so this property is really more of a guideline rather than a necessity.\n",
    "\n",
    "## The Multilayer Perceptron\n",
    "\n",
    "The neuron is the fundamental building block of a neural network. The process of constructing a neural network out of neurons is fairly straightforward: we first construct a __layer__ as an array of neurons, and then we construct a __network__ as a sequence of layers. This architecture is described in the figure below:\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"neural-net\" width=500/>\n",
    "\n",
    "This type of neural network is called a __multilayer perceptron (MLP)__ (or __feed-forward neural network__). The white circles in the figure are invididual neurons (we'll call them __units__ from here on out), and the colored rectangles are layers. There are three types of layers in the figure: __input layers__, __output layers__ and __hidden layers__. Hidden layers and output layers are really the same, the output layer is just the last layer in the network. The input layer is really just a placeholder for the input features (i.e. the white circles in the input layer are just input features, not neurons). We would call the network in this figure a \"3-layer neural network\" (again, we don't count the input layer since it's just a placeholder).\n",
    "\n",
    "The defining aspect of an MLP is how each layer is connected: each unit in a layer receives an input from every unit in the previous layer. A layer with this kind of input connections is called a __fully-connected layer__ (or __dense layer__). Remember that each input to a unit has a weight; just as we represented the set of weights for a single unit as a vector, we can represent all of the input connections to a layer as a __weight matrix__ $W$. This matrix has size $n \\times m$, where $m$ is the number of units in the previous layer and $n$ is the number of units in the given layer, and the $i^{th}$ row is just the weight vector for the $i^{th}$ unit in the given layer. Using this weight matrix we can actually compute the output of every unit in a layer with a single equation:\n",
    "\n",
    "$$\\vec{a} = \\sigma(W \\vec{x} + \\vec{b})$$\n",
    "\n",
    "Where $\\vec{a}$ is a vector of the unit outputs, and we assume that all of the units use the same activation function. Since we're using a lot of vectors and matrices now, here are their sizes:\n",
    "\n",
    "$$\\vec{a}, \\vec{b} \\in \\mathbb{R}^{n}; W \\in \\mathbb{R}^{n \\times m}; \\vec{x} \\in \\mathbb{R}^{m}$$\n",
    "\n",
    "Now that we can construct a layer, compute the output of a layer, and connect layers in sequence, the last step is to compute the output of an entire network from input layer to output layer. This step is actually very simple: the output $\\vec{a}$ of one layer becomes the input $\\vec{x}$ to the next layer. For example, if we consider the 3-layer network from the figure, we can compute the output $\\vec{a}_{out}$ of the network as follows:\n",
    "\n",
    "$$f_i(\\vec{x}) = \\sigma(W_i \\vec{x} + \\vec{b}_i)$$\n",
    "\n",
    "$$\\vec{a}_{out} = f_{net}(\\vec{x}) = f_3(f_2(f_1(\\vec{x})))$$\n",
    "\n",
    "This computation is also called the __forward pass__ of a network.\n",
    "\n",
    "As you can imagine, there really is no limit to the possible size of the network -- you can have as many fully-connected layers as you want, you can have as many units per layer as you want, and you can size each layer indepedently. As a result, the MLP is an extremely general model, meaning that it can model all kinds of functions using a series of matrix transformations and nonlinear activations. This idea is called the __universal approximation theorem__, because the idea is that the MLP is able to _approximate_ any kind of function. Whether the MLP can learn any function _from data_... well, you just have to try it and see.\n",
    "\n",
    "The __network architecture__ -- that is, the number and respective sizes of each layer, and the choice of activation function -- is a major hyperparameter of a neural network. In fact, we've really brought hyperparameters to a new level here, because the architecture could be comprised of an entire list of hyperparameters! This freedom is good because it makes neural networks extremely versatile, but it is also very daunting if we don't know how to pick the right settings for the task. You'll be experimenting with network architecture for your assignment, but for now let's look at some basic applications of neural networks.\n",
    "\n",
    "## Supervised Learning: MLP\n",
    "\n",
    "Let's build a neural network to perform classification on the MNIST dataset. As it turns out, scikit-learn actually has an `MLPClassifier`, but we need something that will run on a GPU -- trust me, we're going to need the extra compute power later on. From now on we're going to use two Python libraries, __Tensorflow__ and __Keras__. Tensorflow is a GPU-accelerated framework for creating and running neural networks, and Keras is a thin wrapper over Tensorflow which provides a much simpler interface (Keras also supports other _backends_ aside from Tensorflow). In short, we will use Keras, but just know that Tensorflow is doing all of the hard work behind the scenes. Now let's make a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about this entire block. It is code we have written to help us access the data really easily!\n",
    "# If you are interested, then go ahead and read it and ask questions later, but for now let's skip it.\n",
    "class data_t(object):\n",
    "    def __init__(self, data, labels):\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "        self.num_examples = data.shape[0]\n",
    "\n",
    "    def next_batch(self, batch_size, index):\n",
    "        idx = index * batch_size\n",
    "        n_idx = index * batch_size + batch_size\n",
    "        return self.data[idx:n_idx, :], self.labels[idx:n_idx, :]\n",
    "\n",
    "class DataContainer:\n",
    "    def __init__(self, data, total_gene_list=None, sub_gene_list=None, train_split=70, test_split=30):\n",
    "        self.num_classes = len(data)\n",
    "        self.label_names_ordered = []\n",
    "        self.class_counts = {}\n",
    "        self.train, self.test = self.split_set(data, total_gene_list, sub_gene_list, train_split, test_split)\n",
    "\n",
    "\n",
    "    #\n",
    "    # USAGE:\n",
    "    # \t\tcreate a new data dictionary with classes as keys that contain a subsample of original data,\n",
    "    #\t\tspecified by the 'sub_gene_list'\n",
    "    # PARAMS:\n",
    "    #\t\torig_data:       dictionary containing classes as keys, with values as matrix of class samples\n",
    "    #\t\t\t\t\t\t with all possible genes\n",
    "    #\t\ttotal_gene_list: list of every gene in the original data GEM\n",
    "    #   \tsub_gene_list:   specified genes from a subset of total gene list\n",
    "    #\n",
    "    def extract_requested_genes(self, orig_data, total_gene_list, sub_gene_list):\n",
    "\n",
    "        #get genes in sub_gen_list from total_gene_list\n",
    "        gene_indexes = []\n",
    "        for i in range(len(sub_gene_list)):\n",
    "            gene_indexes.append(np.argwhere(total_gene_list == sub_gene_list[i]))\n",
    "\n",
    "        # dictionary for requested data\n",
    "        req_data = {}\n",
    "\n",
    "        # iterate through dictionary, replace old data matrix with reduced data matrix\n",
    "        for k in sorted(orig_data.keys()):\n",
    "            reduced_data = np.zeros((len(gene_indexes), orig_data[k].shape[1]))\n",
    "\n",
    "            for idx in xrange(0, len(gene_indexes)):\n",
    "                reduced_data[idx] = orig_data[k][gene_indexes[idx][0],:]\n",
    "\n",
    "            req_data[k] = reduced_data\n",
    "\n",
    "        return req_data\n",
    "\n",
    "\n",
    "    #\n",
    "    # USAGE:\n",
    "    #\tshuffle the data and labels in the same order for a data set and transform the data and labels into numpy arrays\n",
    "    # PARAMS:\n",
    "    #\tdata:\tthe data values for a dataset\n",
    "    # \tlabels: the labels associated with data for a dataset\n",
    "    #\n",
    "    def shuffle_and_transform(self, data, labels):\n",
    "        new_data = []\n",
    "        new_labels = []\n",
    "\n",
    "        samples = random.sample(xrange(len(data)),len(data))\n",
    "\n",
    "        for i in samples:\n",
    "            new_data.append(data[i])\n",
    "            new_labels.append(labels[i])\n",
    "\n",
    "        # convert lists to numpy arrays\n",
    "        np_data = np.asarray(new_data)\n",
    "        np_labels = np.asarray(new_labels)\n",
    "\n",
    "        return data_t(np_data,np_labels)\n",
    "\n",
    "    #\n",
    "    # USAGE:\n",
    "    #\tTODO: What is this use @Colin\n",
    "    def shuffle(self):\n",
    "        idxs = np.arange(self.train.data.shape[0])\n",
    "        idxs = np.random.shuffle(idxs)\n",
    "        self.train.data = np.squeeze(self.train.data[idxs])\n",
    "        self.train.labels = np.squeeze(self.train.labels[idxs])\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    # USAGE:\n",
    "    #       split the data in data dictionary to the specified weights\n",
    "    # PARAMS:\n",
    "    #\t\tdata: \t\t\t dictionary containing all samples of each class, with the class name being the key\n",
    "    #       total_gene_list: list of every sorted gene in the GEM\n",
    "    #       sub_gene_list:   list of genes in the subset wished to be extracted. this can be the total gene list\n",
    "    #\t\t\t\t\t\t if all genes are wanting to be used\n",
    "    #       train_split:     percentage (in integer form) for training class split\n",
    "    #       test_split:      percentage (in integer form) for testing class split\n",
    "    #\n",
    "    def split_set(self, data, total_gene_list=None, sub_gene_list=None, train_split=70, test_split=30):\n",
    "\n",
    "        if test_split + train_split != 100:\n",
    "            print('Test and train split must sum to 100!')\n",
    "            sys.exit(1)\n",
    "\n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        test_data = []\n",
    "        test_labels = []\n",
    "\n",
    "        if sub_gene_list is not None:\n",
    "            data = self.extract_requested_genes(data, total_gene_list, sub_gene_list)\n",
    "\n",
    "        idx = 0 # keep count of index in dictionary\n",
    "\n",
    "        # gather training and testing examples and labels into a list by randomly selecting indices\n",
    "        # of the amount of data in each class\n",
    "        for k in sorted(data.keys()):\n",
    "            self.label_names_ordered.append(k)\n",
    "            self.class_counts[k] = data[k].shape[1]\n",
    "\n",
    "            num_train = int(data[k].shape[1] * train_split / 100)\n",
    "\n",
    "            samples = random.sample(xrange(data[k].shape[1]),data[k].shape[1])\n",
    "            samples_train = samples[0:num_train]\n",
    "            samples_test = samples[num_train:]\n",
    "\n",
    "            for i in xrange(len(samples_train)):\n",
    "                train_data.append(data[k][:,samples_train[i]])\n",
    "\n",
    "                label = np.zeros(self.num_classes)\n",
    "                label[idx] = 1\n",
    "\n",
    "                train_labels.append(label)\n",
    "\n",
    "            for i in xrange(len(samples_test)):\n",
    "                test_data.append(data[k][:,samples_test[i]])\n",
    "\n",
    "                label = np.zeros(self.num_classes)\n",
    "                label[idx] = 1\n",
    "\n",
    "                test_labels.append(label)\n",
    "\n",
    "            idx = idx + 1\n",
    "\n",
    "        train = self.shuffle_and_transform(train_data, train_labels)\n",
    "        test = self.shuffle_and_transform(test_data, test_labels)\n",
    "\n",
    "        return [train, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will help us get the data into a format we can work with and load into the DataContainer\n",
    "def load_data(num_samples_json, gtex_gct_flt):\n",
    "    sample_count_dict = {}\n",
    "    with open(num_samples_json) as f:\n",
    "        sample_count_dict = json.load(f)\n",
    "\n",
    "    idx = 0\n",
    "    data = {}\n",
    "\n",
    "    for k in sorted(sample_count_dict.keys()):\n",
    "        data[k] = gtex_gct_flt[:,idx:(idx + int(sample_count_dict[k]))]\n",
    "        idx = idx + int(sample_count_dict[k])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to load in the gtex GEM from the /zfs directory on palmetto\n",
    "# we also need the list of genes and a JSON that me and csheare created a while pack\n",
    "# that documents the number of samples in each class\n",
    "gtex_gct_flt = np.load('/zfs/feltus/ctargon/gems/gtex_gct_data_float_v7.npy')\n",
    "total_gene_list = np.load('/zfs/feltus/ctargon/gene_lists/gtex_gene_list_v7.npy')\n",
    "data = load_data('/zfs/feltus/ctargon/class_counts/gtex_tissue_count_v7.json', gtex_gct_flt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now going to load the data into the DataContainer object so we can easily access it...\n",
    "dataset = DataContainer(data, total_gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some metavariables we might use later...\n",
    "num_classes = len(data) # data variable is a dictionary with each key being a class\n",
    "num_genes = dataset.train.data.shape[-1] # train.data is num_samples x num_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it is time to create our model, preprocess our data, and get classifying\n",
    "# create a neural network with three hidden layers\n",
    "mlp = keras.models.Sequential()\n",
    "mlp.add(keras.layers.Dense(units=1024, activation=\"relu\", input_shape=(num_genes,))) \n",
    "mlp.add(keras.layers.Dense(units=512, activation=\"relu\"))\n",
    "mlp.add(keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "mlp.add(keras.layers.Dense(units=num_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compile the model and check out the stats\n",
    "# compile the model\n",
    "mlp.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# print a summary of the model\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do a little preprocessing, then fit the model\n",
    "scaler = preprocessing.MinMaxScaler() #preprocessing.MaxAbsScaler()\n",
    "dataset.train.data = scaler.fit_transform(dataset.train.data)\n",
    "dataset.test.data = scaler.fit_transform(dataset.test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = mlp.fit(x=dataset.train.data, y=dataset.train.labels, batch_size=128, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training accuracy\n",
    "plt.plot(history.history[\"acc\"])\n",
    "plt.plot(history.history[\"val_acc\"])\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# plot the training loss\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "scores = mlp.evaluate(x=dataset.test.data, y=dataset.test.labels)\n",
    "\n",
    "# print results\n",
    "for name, score in zip(mlp.metrics_names, scores):\n",
    "    print(\"%s: %g\" % (name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf kernel",
   "language": "python",
   "name": "python_custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
